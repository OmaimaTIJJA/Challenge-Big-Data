{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmaimaTIJJA/Challenge-Big-Data/blob/main/projet_bigdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQkwWnaUs1xb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Define the URL to scrape\n",
        "url=\"https://www.hespress.com/%d9%88%d9%81%d8%a7%d8%a9-4-%d9%85%d8%ba%d8%a7%d8%b1%d8%a8%d8%a9-%d8%a5%d8%ab%d8%b1-%d8%a7%d9%84%d8%b2%d9%84%d8%b2%d8%a7%d9%84-%d8%a7%d9%84%d8%b9%d9%86%d9%8a%d9%81-%d8%a8%d8%aa%d8%b1%d9%83%d9%8a%d8%a7-1120395.html\"# Make a GET request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract the comments from the HTML\n",
        "comments = soup.find_all(\"div\", class_=\"comments\")\n",
        "\n",
        "# Connect to the MongoDB instance\n",
        "client = MongoClient(\"mongodb+srv://Oumaima:Oumaima1234@cluster0.ac37b4t.mongodb.net/?retryWrites=true&w=majority\")\n",
        "\n",
        "# Get the comments collection\n",
        "comments_collection = client.test.comments\n",
        "\n",
        "comments_to_insert = []\n",
        "# Iterate through the comments and extract the text of the two div elements\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_obj = {\"author\": div1.text.split(\"\\n\")[2], \"text\": div2.text}\n",
        "            comments_to_insert.append(comment_obj)\n",
        "\n",
        "# insert all comments\n",
        "if len(comments_to_insert) > 0:\n",
        "    comments_collection.insert_many(comments_to_insert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSSjkxtMs0Nw",
        "outputId": "50f34f97-a48d-463e-abb5-3181917bcef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kafka.python in /usr/local/lib/python3.8/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install kafka.python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqaARt4Ry_hd"
      },
      "outputs": [],
      "source": [
        "from confluent_kafka import Producer, Consumer\n",
        "import json\n",
        "\n",
        "# Kafka producer\n",
        "producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
        "\n",
        "# Iterate through the comments and extract the text of the two div elements\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_obj = {\"author\": div1.text.split(\"\\n\")[2], \"text\": div2.text}\n",
        "            comment_json = json.dumps(comment_obj)\n",
        "            producer.produce('comment', value=bytes(comment_json, 'utf-8'))\n",
        "            producer.flush()\n",
        "            print(f'Published message: {comment_json}')\n",
        "\n",
        "\n",
        "# Kafka consumer\n",
        "conf = {'bootstrap.servers': 'localhost:9092',\n",
        "        'group.id': 'mygroup',\n",
        "        'auto.offset.reset': 'earliest'}\n",
        "\n",
        "consumer = Consumer(conf)\n",
        "\n",
        "consumer.subscribe(['comment'])\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
        "                msg.topic(), msg.partition(), msg.offset()))\n",
        "        else:\n",
        "            print('Error occured: {}'.format(msg.error()))\n",
        "    else:\n",
        "        comment = json.loads(msg.value())\n",
        "        print(comment)\n",
        "consumer.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from confluent_kafka import Producer, Consumer\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# download the necessary NLTK resources\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Create a SentimentIntensityAnalyzer object\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Iterate through the comments and extract the text of the two div elements\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_obj = {\"author\": div1.text, \"text\": div2.text}\n",
        "            comment_text = comment_obj[\"text\"]\n",
        "            sentiment = sia.polarity_scores(comment_text)\n",
        "            print(\"Comment text: \", comment_text)\n",
        "            print(\"Sentiment: \", sentiment)\n",
        "# Kafka consumer\n",
        "conf = {'bootstrap.servers': 'localhost:9092',\n",
        "        'group.id': 'mygroup',\n",
        "        'auto.offset.reset': 'earliest'}\n",
        "\n",
        "consumer = Consumer(conf)\n",
        "\n",
        "consumer.subscribe(['comment'])\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
        "                msg.topic(), msg.partition(), msg.offset()))\n",
        "        else:\n",
        "            print('Error occured: {}'.format(msg.error()))\n",
        "    else:\n",
        "        comment = json.loads(msg.value())\n",
        "        print(comment)\n",
        "# close the consumer after the loop\n",
        "consumer.close()"
      ],
      "metadata": {
        "id": "GmdfFKXA7Ibl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x-6y8uo6xti-"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_text = div2.text\n",
        "            blob = TextBlob(comment_text)\n",
        "            sentiment = blob.sentiment\n",
        "            print(sentiment)\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
        "                msg.topic(), msg.partition(), msg.offset()))\n",
        "        else:\n",
        "            print('Error occured: {}'.format(msg.error()))\n",
        "    else:\n",
        "        comment = json.loads(msg.value())\n",
        "        print(comment)\n",
        "# close the consumer after the loop\n",
        "consumer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rilwvxepx8ru"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N-v3zBEwyAKV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\21263\\Downloads\\comments.json\")\n",
        "\n",
        "# Perform batch processing on the data\n",
        "result = df.rdd\\\n",
        "    .map(lambda x: (x.author, x.text))\\\n",
        "    .groupByKey()\\\n",
        "    .mapValues(list)\\\n",
        "    .collect()\n",
        "\n",
        "# Show result\n",
        "for author, text in result:\n",
        "    print(f\"Author: {author}\\nText: {text}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XhWdOTcPyOSG"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\21263\\Downloads\\comments.json\")\n",
        "\n",
        "# Specify the batch size for processing\n",
        "batch_size = 82\n",
        "\n",
        "# Get the number of records in the DataFrame\n",
        "num_records = df.count()\n",
        "print(num_records)\n",
        "# Calculate the number of batches to process\n",
        "num_batches = (num_records + batch_size - 1) // batch_size\n",
        "print(num_batches)\n",
        "\n",
        "# Loop through the batches and process the data\n",
        "for i in range(num_batches):\n",
        "    # Get the start and end index for the current batch\n",
        "    start_index = i * batch_size\n",
        "    end_index = min(start_index + batch_size, num_records)\n",
        "    \n",
        "    # Get the DataFrame for the current batch\n",
        "    batch_df = df.limit(batch_size).coalesce(1)\n",
        "    \n",
        "    \n",
        "    # Perform batch processing on the data\n",
        "    result = batch_df.rdd\\\n",
        "        .map(lambda x: (x.author, x.text))\\\n",
        "        .groupByKey()\\\n",
        "        .mapValues(list)\\\n",
        "        .collect()\n",
        "    \n",
        "    # Show result\n",
        "    for author, text in result:\n",
        "        print(f\"Author: {author}\\nText: {text}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C5OGqgrdyfJg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\21263\\Downloads\\comments.json\")\n",
        "\n",
        "# Perform batch processing on the data\n",
        "result = df.rdd\\\n",
        "    .map(lambda x: (x.author, x.text))\\\n",
        "    .groupByKey()\\\n",
        "    .mapValues(list)\\\n",
        "    .collect()\n",
        "\n",
        "# Perform sentiment analysis using TextBlob\n",
        "for author, text in result:\n",
        "    sentiment_scores = [TextBlob(t).sentiment.polarity for t in text]\n",
        "    average_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
        "    if average_sentiment != 0:\n",
        "        print(f\"Author: {author}\\nAverage sentiment: {average_sentiment}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RpCtCVIfyhe6"
      },
      "outputs": [],
      "source": [
        "from operator import add\n",
        "import re\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, desc\n",
        "\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\21263\\Downloads\\comments.json\")\n",
        "\n",
        "# Split the \"text\" field into individual words\n",
        "df = df.select(\"author\", explode(split(df[\"text\"], \" \")).alias(\"word\"))\n",
        "\n",
        "# Group by word and count occurrences\n",
        "word_counts = df.groupBy(\"word\").count().sort(desc(\"count\"))\n",
        "\n",
        "# Show the most commonly used words\n",
        "word_counts.show(10)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb4HS3jWp3MurqVlHCTp+u",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}